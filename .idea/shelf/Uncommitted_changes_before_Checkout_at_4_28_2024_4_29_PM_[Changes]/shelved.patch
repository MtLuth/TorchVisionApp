Index: app/src/main/java/com/example/torchvisionapp/ocr/OCRActivity.kt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package com.example.torchvisionapp.ocr\r\n\r\nimport android.Manifest\r\nimport android.annotation.SuppressLint\r\nimport android.app.Activity\r\nimport android.content.Context\r\nimport android.content.Intent\r\nimport android.content.pm.PackageManager\r\nimport android.content.res.ColorStateList\r\nimport android.graphics.Bitmap\r\nimport android.graphics.BitmapFactory\r\nimport android.net.Uri\r\nimport android.os.Bundle\r\nimport android.provider.MediaStore\r\nimport android.util.Log\r\nimport android.view.MotionEvent\r\nimport android.view.View\r\nimport android.widget.Button\r\nimport android.widget.EditText\r\nimport android.widget.ImageView\r\nimport android.widget.TextView\r\nimport android.widget.Toast\r\nimport androidx.appcompat.app.AppCompatActivity\r\nimport androidx.core.app.ActivityCompat\r\nimport androidx.core.content.ContextCompat\r\nimport androidx.lifecycle.Observer\r\nimport androidx.lifecycle.ViewModelProvider.AndroidViewModelFactory\r\nimport com.bumptech.glide.Glide\r\nimport com.example.torchvisionapp.R\r\nimport com.google.android.material.chip.Chip\r\nimport kotlinx.coroutines.MainScope\r\nimport kotlinx.coroutines.asCoroutineDispatcher\r\nimport kotlinx.coroutines.async\r\nimport kotlinx.coroutines.sync.Mutex\r\nimport kotlinx.coroutines.sync.withLock\r\nimport org.json.JSONException\r\nimport org.json.JSONObject\r\nimport org.pytorch.IValue\r\nimport org.pytorch.LiteModuleLoader\r\nimport org.pytorch.Module\r\nimport org.pytorch.Tensor\r\nimport java.io.ByteArrayOutputStream\r\nimport java.io.File\r\nimport java.io.FileOutputStream\r\nimport java.io.IOException\r\nimport java.io.InputStream\r\nimport java.nio.LongBuffer\r\nimport java.util.concurrent.Executors\r\n\r\nprivate const val TAG = \"MainActivity\"\r\n\r\npublic class OCRActivity : AppCompatActivity() {\r\n\r\n    private val tfImageName = \"tensorflow.jpg\"\r\n    private val androidImageName = \"android.jpg\"\r\n    private val chromeImageName = \"chrome.jpg\"\r\n    private lateinit var viewModel: MLExecutionViewModel\r\n    private lateinit var resultImageView: ImageView\r\n\r\n    //    private lateinit var tfImageView: ImageView\r\n//    private lateinit var androidImageView: ImageView\r\n//    private lateinit var chromeImageView: ImageView\r\n//    private lateinit var chipsGroup: ChipGroup\r\n    private lateinit var runButton: Button\r\n    private lateinit var textPromptTextView: TextView\r\n\r\n    private var useGPU = false\r\n    private var selectedImageName = \"tensorflow.jpg\"\r\n    private var ocrModel: OCRModelExecutor? = null\r\n    private val inferenceThread = Executors.newSingleThreadExecutor().asCoroutineDispatcher()\r\n    private val mainScope = MainScope()\r\n    private val mutex = Mutex()\r\n\r\n    //\r\n//    private lateinit var viewModel: MLExecutionViewModel\r\n    private lateinit var selectedImageUri: Uri\r\n    private val REQUEST_IMAGE_CAPTURE = 1\r\n    private val REQUEST_IMAGE_PICK = 2\r\n    private val REQUEST_CAMERA_PERMISSION = 200\r\n\r\n    private lateinit var fransText: TextView\r\n    private lateinit var engText: TextView\r\n\r\n    //    fields for translate\r\n    private val HIDDEN_SIZE: Int = 256\r\n    private val EOS_TOKEN = 1\r\n    private val MAX_LENGTH = 50\r\n    private var mModuleEncoder: Module? = null\r\n    private var mModuleDecoder: Module? = null\r\n    private var mInputTensor: Tensor? = null\r\n    private var mInputTensorBuffer: LongBuffer? = null\r\n\r\n    //    property for display UI\r\n    private var mEditText: EditText? = null\r\n    private var mTextView: TextView? = null\r\n    private lateinit var mButton: Button\r\n\r\n\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n        super.onCreate(savedInstanceState)\r\n//        setContentView(R.layout.tfe_is_activity_main)\r\n        setContentView(R.layout.activity_translate)\r\n\r\n        // Check if the app has permission to use the camera\r\n        if (ContextCompat.checkSelfPermission(\r\n                this, Manifest.permission.CAMERA\r\n            ) != PackageManager.PERMISSION_GRANTED\r\n        ) {\r\n            // If not, request the permission\r\n            ActivityCompat.requestPermissions(\r\n                this, arrayOf(Manifest.permission.CAMERA), REQUEST_CAMERA_PERMISSION\r\n            )\r\n        }\r\n\r\n        fransText = findViewById(R.id.recordText1)\r\n        engText = findViewById(R.id.recordText2)\r\n\r\n        mButton = findViewById(R.id.btnTranslate)\r\n\r\n        mButton.setOnClickListener {\r\n            translate(fransText.toString())\r\n        }\r\n\r\n        val buttonPickImage: Button = findViewById(R.id.button_pick_image)\r\n        buttonPickImage.setOnClickListener {\r\n            dispatchPickPictureIntent()\r\n        }\r\n\r\n        val buttonCaptureImage: Button = findViewById(R.id.button_capture_image)\r\n        buttonCaptureImage.setOnClickListener {\r\n            dispatchTakePictureIntent()\r\n        }\r\n\r\n//        val toolbar: Toolbar = findViewById(R.id.toolbar)\r\n//        setSupportActionBar(toolbar)\r\n//        supportActionBar?.setDisplayShowTitleEnabled(false)\r\n\r\n//        tfImageView = findViewById(R.id.tf_imageview)\r\n//        androidImageView = findViewById(R.id.android_imageview)\r\n//        chromeImageView = findViewById(R.id.chrome_imageview)\r\n\r\n//        val candidateImageViews = arrayOf<ImageView>(tfImageView, androidImageView, chromeImageView)\r\n\r\n        val assetManager = assets\r\n        try {\r\n            val tfInputStream: InputStream = assetManager.open(tfImageName)\r\n            val tfBitmap = BitmapFactory.decodeStream(tfInputStream)\r\n//            tfImageView.setImageBitmap(tfBitmap)\r\n\r\n            val androidInputStream: InputStream = assetManager.open(androidImageName)\r\n            val androidBitmap = BitmapFactory.decodeStream(androidInputStream)\r\n//            androidImageView.setImageBitmap(androidBitmap)\r\n\r\n            val chromeInputStream: InputStream = assetManager.open(chromeImageName)\r\n            val chromeBitmap = BitmapFactory.decodeStream(chromeInputStream)\r\n//            chromeImageView.setImageBitmap(chromeBitmap)\r\n        } catch (e: IOException) {\r\n            Log.e(TAG, \"Failed to open a test image\")\r\n        }\r\n\r\n//        for (iv in candidateImageViews) {\r\n//            setInputImageViewListener(iv)\r\n//        }\r\n\r\n        resultImageView = findViewById(R.id.result_imageview)\r\n//        chipsGroup = findViewById(R.id.chips_group)\r\n//        textPromptTextView = findViewById(R.id.text_prompt)\r\n//        val useGpuSwitch: Switch = findViewById(R.id.switch_use_gpu)\r\n\r\n        viewModel = AndroidViewModelFactory(application).create(MLExecutionViewModel::class.java)\r\n        viewModel.resultingBitmap.observe(this, Observer { resultImage ->\r\n            if (resultImage != null) {\r\n                updateUIWithResults(resultImage)\r\n            }\r\n            enableControls(true)\r\n        })\r\n\r\n        mainScope.async(inferenceThread) { createModelExecutor(useGPU) }\r\n\r\n//        useGpuSwitch.setOnCheckedChangeListener { _, isChecked ->\r\n//            useGPU = isChecked\r\n//            mainScope.async(inferenceThread) { createModelExecutor(useGPU) }\r\n//        }\r\n\r\n//        runButton = findViewById(R.id.rerun_button)\r\n//        runButton.setOnClickListener {\r\n//            enableControls(false)\r\n//\r\n//            mainScope.async(inferenceThread) {\r\n//                mutex.withLock {\r\n//                    if (ocrModel != null) {\r\n//                        viewModel.onApplyModel(\r\n//                            baseContext,\r\n//                            selectedImageName,\r\n//                            ocrModel,\r\n//                            inferenceThread\r\n//                        )\r\n//                    } else {\r\n//                        Log.d(\r\n//                            TAG,\r\n//                            \"Skipping running OCR since the ocrModel has not been properly initialized ...\"\r\n//                        )\r\n//                    }\r\n//                }\r\n//            }\r\n//        }\r\n\r\n//\r\n        runButton = findViewById(R.id.rerun_button)\r\n        runButton.setOnClickListener {\r\n            enableControls(false)\r\n\r\n            mainScope.async(inferenceThread) {\r\n                mutex.withLock {\r\n                    if (ocrModel != null) {\r\n                        viewModel.onApplyModel(\r\n                            baseContext, selectedImageUri, ocrModel, inferenceThread\r\n                        )\r\n                    } else {\r\n                        Log.d(\r\n                            TAG,\r\n                            \"Skipping running OCR since the ocrModel has not been properly initialized ...\"\r\n                        )\r\n                    }\r\n                }\r\n            }\r\n        }\r\n//\r\n\r\n        setChipsToLogView(HashMap<String, Int>())\r\n        enableControls(true)\r\n    }\r\n\r\n    //\r\n    override fun onRequestPermissionsResult(\r\n        requestCode: Int, permissions: Array<String>, grantResults: IntArray\r\n    ) {\r\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults)\r\n        when (requestCode) {\r\n            REQUEST_CAMERA_PERMISSION -> {\r\n                if (grantResults.isNotEmpty() && grantResults[0] == PackageManager.PERMISSION_GRANTED) {\r\n                    // Permission was granted, you can now use the camera\r\n                } else {\r\n                    // Permission was denied, show a message to the user\r\n                    Toast.makeText(\r\n                        this,\r\n                        \"Camera permission is required to use this feature\",\r\n                        Toast.LENGTH_SHORT\r\n                    ).show()\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    fun dispatchTakePictureIntent() {\r\n        Intent(MediaStore.ACTION_IMAGE_CAPTURE).also { takePictureIntent ->\r\n            takePictureIntent.resolveActivity(packageManager)?.also {\r\n                startActivityForResult(takePictureIntent, REQUEST_IMAGE_CAPTURE)\r\n            }\r\n        }\r\n    }\r\n\r\n    fun dispatchPickPictureIntent() {\r\n        Intent(\r\n            Intent.ACTION_PICK, MediaStore.Images.Media.EXTERNAL_CONTENT_URI\r\n        ).also { pickPictureIntent ->\r\n            pickPictureIntent.resolveActivity(packageManager)?.also {\r\n                startActivityForResult(pickPictureIntent, REQUEST_IMAGE_PICK)\r\n            }\r\n        }\r\n    }\r\n\r\n    override fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?) {\r\n        super.onActivityResult(requestCode, resultCode, data)\r\n\r\n        if (resultCode == Activity.RESULT_OK) {\r\n            when (requestCode) {\r\n                REQUEST_IMAGE_CAPTURE -> {\r\n                    val imageBitmap = data?.extras?.get(\"data\") as Bitmap\r\n                    // Convert bitmap to Uri\r\n                    selectedImageUri = getImageUriFromBitmap(this, imageBitmap)\r\n                    setImageView(resultImageView, imageBitmap)\r\n                }\r\n\r\n                REQUEST_IMAGE_PICK -> {\r\n                    selectedImageUri = data?.data!!\r\n\r\n                    val imageBitmap = getBitmapFromUri(this, selectedImageUri)\r\n                    setImageView(resultImageView, imageBitmap)\r\n\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    fun getBitmapFromUri(context: Context, uri: Uri): Bitmap {\r\n        return context.contentResolver.openInputStream(uri)?.use { inputStream ->\r\n            BitmapFactory.decodeStream(inputStream)\r\n        } ?: throw IllegalArgumentException(\"Cannot decode bitmap from uri: $uri\")\r\n    }\r\n\r\n    fun getImageUriFromBitmap(inContext: Context, inImage: Bitmap): Uri {\r\n        val bytes = ByteArrayOutputStream()\r\n        inImage.compress(Bitmap.CompressFormat.JPEG, 100, bytes)\r\n        val path =\r\n            MediaStore.Images.Media.insertImage(inContext.contentResolver, inImage, \"Title\", null)\r\n        return Uri.parse(path)\r\n    }\r\n\r\n\r\n//\r\n\r\n    @SuppressLint(\"ClickableViewAccessibility\")\r\n    private fun setInputImageViewListener(iv: ImageView) {\r\n        iv.setOnTouchListener(object : View.OnTouchListener {\r\n            override fun onTouch(v: View, event: MotionEvent?): Boolean {\r\n//                    if (v.equals(tfImageView)) {\r\n//                        selectedImageName = tfImageName\r\n////                        textPromptTextView.setText(getResources().getString(R.string.tfe_using_first_image))\r\n//                    } else if (v.equals(androidImageView)) {\r\n//                        selectedImageName = androidImageName\r\n////                        textPromptTextView.setText(getResources().getString(R.string.tfe_using_second_image))\r\n//                    } else if (v.equals(chromeImageView)) {\r\n//                        selectedImageName = chromeImageName\r\n////                        textPromptTextView.setText(getResources().getString(R.string.tfe_using_third_image))\r\n//                    }\r\n                return false\r\n            }\r\n        })\r\n    }\r\n\r\n    private suspend fun createModelExecutor(useGPU: Boolean) {\r\n        mutex.withLock {\r\n            if (ocrModel != null) {\r\n                ocrModel!!.close()\r\n                ocrModel = null\r\n            }\r\n            try {\r\n                ocrModel = OCRModelExecutor(this, useGPU)\r\n            } catch (e: Exception) {\r\n                Log.e(TAG, \"Fail to create OCRModelExecutor: ${e.message}\")\r\n//                val logText: TextView = findViewById(R.id.log_view)\r\n//                logText.text = e.message\r\n            }\r\n        }\r\n    }\r\n\r\n    private fun setChipsToLogView(itemsFound: Map<String, Int>) {\r\n//        chipsGroup.removeAllViews()\r\n\r\n        for ((word, color) in itemsFound) {\r\n            val chip = Chip(this)\r\n            chip.text = word\r\n            chip.chipBackgroundColor = getColorStateListForChip(color)\r\n            chip.isClickable = false\r\n//            chipsGroup.addView(chip)\r\n        }\r\n//        val labelsFoundTextView: TextView = findViewById(R.id.tfe_is_labels_found)\r\n//        if (chipsGroup.childCount == 0) {\r\n//            labelsFoundTextView.text = getString(R.string.tfe_ocr_no_text_found)\r\n//        } else {\r\n//            labelsFoundTextView.text = getString(R.string.tfe_ocr_texts_found)\r\n//        }\r\n//        chipsGroup.parent.requestLayout()\r\n    }\r\n\r\n    private fun getColorStateListForChip(color: Int): ColorStateList {\r\n        val states = arrayOf(\r\n            intArrayOf(android.R.attr.state_enabled), // enabled\r\n            intArrayOf(android.R.attr.state_pressed) // pressed\r\n        )\r\n\r\n        val colors = intArrayOf(color, color)\r\n        return ColorStateList(states, colors)\r\n    }\r\n\r\n    private fun setImageView(imageView: ImageView, image: Bitmap) {\r\n        Glide.with(baseContext).load(image).override(250, 250).fitCenter().into(imageView)\r\n    }\r\n\r\n    private fun updateUIWithResults(modelExecutionResult: ModelExecutionResult) {\r\n        setImageView(resultImageView, modelExecutionResult.bitmapResult)\r\n//        val logText: TextView = findViewById(R.id.log_view)\r\n//        logText.text = modelExecutionResult.executionLog\r\n\r\n        setChipsToLogView(modelExecutionResult.itemsFound)\r\n\r\n        for ((word) in modelExecutionResult.itemsFound) {\r\n            Log.d(\"Recognition result:\", word)\r\n        }\r\n\r\n        var resultString = \"\"\r\n        val itemsList = modelExecutionResult.itemsFound.toList()\r\n        val reversedItems = itemsList.reversed()\r\n\r\n        for ((word) in reversedItems) {\r\n            Log.d(\"Recognition result:\", word)\r\n            resultString += \"$word, \"\r\n        }\r\n\r\n        fransText.setText(resultString)\r\n        Log.d(\"OCR\", modelExecutionResult.itemsFound.toString())\r\n\r\n        enableControls(true)\r\n    }\r\n\r\n    private fun enableControls(enable: Boolean) {\r\n        runButton.isEnabled = enable\r\n    }\r\n\r\n    //    handle machine translate\r\n//    func to get model file\r\n    @Throws(IOException::class)\r\n    private fun assetFilePath(context: Context, assetName: String): String? {\r\n        val file = File(context.filesDir, assetName)\r\n        if (file.exists() && file.length() > 0) {\r\n            return file.absolutePath\r\n        }\r\n        context.assets.open(assetName).use { `is` ->\r\n            FileOutputStream(file).use { os ->\r\n                val buffer = ByteArray(4 * 1024)\r\n                var read: Int\r\n                while (`is`.read(buffer).also { read = it } != -1) {\r\n                    os.write(buffer, 0, read)\r\n                }\r\n                os.flush()\r\n            }\r\n            return file.absolutePath\r\n        }\r\n    }\r\n\r\n\r\n    //    func to display result\r\n    private fun showTranslationResult(result: String?) {\r\n        engText.setText(result)\r\n    }\r\n\r\n    fun run() {\r\n        val result = translate(fransText.getText().toString())\r\n        runOnUiThread {\r\n            showTranslationResult(result)\r\n            mButton?.setEnabled(true)\r\n        }\r\n    }\r\n\r\n    //    func handle logic translate, including load model and translate by use model to predict\r\n    private fun translate(text: String): String? {\r\n        if (mModuleEncoder == null) {\r\n            try {\r\n                mModuleEncoder = LiteModuleLoader.load(\r\n                    assetFilePath(\r\n                        applicationContext, \"models/machine_translate/optimized_encoder_150k.ptl\"\r\n                    )\r\n                )\r\n            } catch (e: IOException) {\r\n                Log.e(\"TAG\", \"Error reading assets\", e)\r\n                finish()\r\n            }\r\n        }\r\n        var json: String\r\n        val wrd2idx: JSONObject\r\n        val idx2wrd: JSONObject\r\n        try {\r\n            var `is` = assets.open(\"models/machine_translate/target_idx2wrd.json\")\r\n            var size = `is`.available()\r\n            var buffer = ByteArray(size)\r\n            `is`.read(buffer)\r\n            `is`.close()\r\n            json = String(buffer, charset(\"UTF-8\"))\r\n            idx2wrd = JSONObject(json)\r\n            `is` = assets.open(\"models/machine_translate/source_wrd2idx.json\")\r\n            size = `is`.available()\r\n            buffer = ByteArray(size)\r\n            `is`.read(buffer)\r\n            `is`.close()\r\n            json = String(buffer, charset(\"UTF-8\"))\r\n            wrd2idx = JSONObject(json)\r\n        } catch (e: JSONException) {\r\n            Log.e(\r\n                \"TAG\", \"JSONException | IOException \", e\r\n            )\r\n            return null\r\n        } catch (e: IOException) {\r\n            Log.e(\r\n                \"TAG\", \"JSONException | IOException \", e\r\n            )\r\n            return null\r\n        }\r\n        val inputs =\r\n            LongArray(text.split(\" \".toRegex()).dropLastWhile { it.isEmpty() }.toTypedArray().size)\r\n        try {\r\n            for (i in text.split(\" \".toRegex()).dropLastWhile { it.isEmpty() }\r\n                .toTypedArray().indices) {\r\n                inputs[i] = wrd2idx.getLong(text.split(\" \".toRegex()).dropLastWhile { it.isEmpty() }\r\n                    .toTypedArray()[i])\r\n            }\r\n        } catch (e: JSONException) {\r\n            Log.e(\"machine_translation\", \"JSONException \", e)\r\n            return null\r\n        }\r\n        val inputShape = longArrayOf(1)\r\n        val hiddenShape = longArrayOf(1, 1, 256)\r\n        val hiddenTensorBuffer = Tensor.allocateFloatBuffer(1 * 1 * 256)\r\n        var hiddenTensor = Tensor.fromBlob(hiddenTensorBuffer, hiddenShape)\r\n        val outputsShape = longArrayOf(\r\n            50,\r\n            256\r\n        )\r\n        val outputsTensorBuffer =\r\n            Tensor.allocateFloatBuffer(50 * 256)\r\n        for (i in inputs.indices) {\r\n            val inputTensorBuffer = Tensor.allocateLongBuffer(1)\r\n            inputTensorBuffer.put(inputs[i])\r\n            val inputTensor = Tensor.fromBlob(inputTensorBuffer, inputShape)\r\n            val outputTuple: Array<IValue> =\r\n                mModuleEncoder!!.forward(IValue.from(inputTensor), IValue.from(hiddenTensor))\r\n                    .toTuple()\r\n            val outputTensor = outputTuple[0].toTensor()\r\n            outputsTensorBuffer.put(outputTensor.dataAsFloatArray)\r\n            hiddenTensor = outputTuple[1].toTensor()\r\n        }\r\n        val outputsTensor = Tensor.fromBlob(outputsTensorBuffer, outputsShape)\r\n        val decoderInputShape = longArrayOf(1, 1)\r\n        if (mModuleDecoder == null) {\r\n            try {\r\n                mModuleDecoder = LiteModuleLoader.load(\r\n                    assetFilePath(\r\n                        applicationContext, \"models/machine_translate/optimized_decoder_150k.ptl\"\r\n                    )\r\n                )\r\n            } catch (e: IOException) {\r\n                Log.e(\"machine_translation\", \"Error reading assets\", e)\r\n                finish()\r\n            }\r\n        }\r\n//        mInputTensorBuffer = Tensor.allocateLongBuffer(1)\r\n//        mInputTensorBuffer.put(0)\r\n        var mInputTensorBuffer = Tensor.allocateLongBuffer(1)\r\n        val localBuffer = mInputTensorBuffer\r\n        localBuffer.put(0)\r\n        mInputTensorBuffer = localBuffer\r\n\r\n        mInputTensor = Tensor.fromBlob(mInputTensorBuffer, decoderInputShape)\r\n        val result: ArrayList<Int> =\r\n            ArrayList<Int>(50)\r\n        for (i in 0 until 50) {\r\n            val outputTuple: Array<IValue> = mModuleDecoder!!.forward(\r\n                IValue.from(mInputTensor), IValue.from(hiddenTensor), IValue.from(outputsTensor)\r\n            ).toTuple()\r\n            val decoderOutputTensor = outputTuple[0].toTensor()\r\n            hiddenTensor = outputTuple[1].toTensor()\r\n            val outputs = decoderOutputTensor.dataAsFloatArray\r\n            var topIdx = 0\r\n            var topVal = -Double.MAX_VALUE\r\n            for (j in outputs.indices) {\r\n                if (outputs[j] > topVal) {\r\n                    topVal = outputs[j].toDouble()\r\n                    topIdx = j\r\n                }\r\n            }\r\n            if (topIdx == 1) break\r\n            result.add(topIdx)\r\n//            mInputTensorBuffer = Tensor.allocateLongBuffer(1)\r\n//            mInputTensorBuffer.put(topIdx.toLong())\r\n            var mInputTensorBuffer = Tensor.allocateLongBuffer(1)\r\n            val localBuffer = mInputTensorBuffer\r\n            localBuffer.put(topIdx.toLong())\r\n            mInputTensorBuffer = localBuffer\r\n\r\n            mInputTensor = Tensor.fromBlob(mInputTensorBuffer, decoderInputShape)\r\n        }\r\n        var english = \"\"\r\n        try {\r\n            for (i in result.indices) english += \" \" + idx2wrd.getString(\"\" + result[i])\r\n        } catch (e: JSONException) {\r\n            Log.e(\"machine_translation\", \"JSONException \", e)\r\n        }\r\n        return english\r\n    }\r\n\r\n}\r\n\r\n
===================================================================
diff --git a/app/src/main/java/com/example/torchvisionapp/ocr/OCRActivity.kt b/app/src/main/java/com/example/torchvisionapp/ocr/OCRActivity.kt
--- a/app/src/main/java/com/example/torchvisionapp/ocr/OCRActivity.kt	
+++ b/app/src/main/java/com/example/torchvisionapp/ocr/OCRActivity.kt	
@@ -27,6 +27,7 @@
 import androidx.lifecycle.ViewModelProvider.AndroidViewModelFactory
 import com.bumptech.glide.Glide
 import com.example.torchvisionapp.R
+import com.google.android.datatransport.runtime.dagger.Module
 import com.google.android.material.chip.Chip
 import kotlinx.coroutines.MainScope
 import kotlinx.coroutines.asCoroutineDispatcher
